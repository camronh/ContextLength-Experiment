{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pandas tiktoken langchain_core langchain-openai langchain-google-genai matplotlib langsmith python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithUserError",
     "evalue": "API key must be provided when using hosted LangSmith API",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLangSmithUserError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      5\u001b[0m load_dotenv()\n\u001b[0;32m----> 7\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m app_data_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/AppleStore.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m descriptions_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/appleStore_description.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Dev/AI-Research/.venv/lib/python3.10/site-packages/langsmith/client.py:535\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, api_url, api_key, retry_config, timeout_ms, web_url, session, auto_batch_tracing, hide_inputs, hide_outputs, info, api_urls)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_url \u001b[38;5;241m=\u001b[39m _get_api_url(api_url)\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m _get_api_key(api_key)\n\u001b[0;32m--> 535\u001b[0m     \u001b[43m_validate_api_key_if_hosted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_api_urls \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_url: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key}\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_config \u001b[38;5;241m=\u001b[39m retry_config \u001b[38;5;129;01mor\u001b[39;00m _default_retry_config()\n",
      "File \u001b[0;32m~/Documents/Dev/AI-Research/.venv/lib/python3.10/site-packages/langsmith/client.py:324\u001b[0m, in \u001b[0;36m_validate_api_key_if_hosted\u001b[0;34m(api_url, api_key)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_langchain_hosted(api_url):\n\u001b[0;32m--> 324\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithUserError(\n\u001b[1;32m    325\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI key must be provided when using hosted LangSmith API\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         )\n",
      "\u001b[0;31mLangSmithUserError\u001b[0m: API key must be provided when using hosted LangSmith API"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langsmith import Client\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Client()\n",
    "\n",
    "app_data_df = pd.read_csv('./data/AppleStore.csv')\n",
    "\n",
    "descriptions_df = pd.read_csv('./data/appleStore_description.csv')\n",
    "\n",
    "full_app_df = pd.merge(app_data_df, descriptions_df, on='id', how='left')\n",
    "full_app_df.head()\n",
    "\n",
    "\n",
    "# Assuming combined_df is already defined\n",
    "# Step 1: Select only the required columns\n",
    "new_df = full_app_df[['id', 'track_name_x', 'size_bytes_x', 'currency',\n",
    "                      'price', 'rating_count_tot', 'user_rating', 'ver', 'prime_genre', 'app_desc']]\n",
    "\n",
    "# Step 2: Rename columns\n",
    "new_df = new_df.rename(\n",
    "    columns={'track_name_x': 'name', 'size_bytes_x': 'size'})\n",
    "\n",
    "# Step 3: Convert size from bytes to MB\n",
    "new_df['size'] = new_df['size'] / (1024 * 1024)  # 1 MB = 1024 * 1024 bytes\n",
    "\n",
    "\n",
    "# Remove special characters from the 'name' field\n",
    "new_df['name'] = new_df['name'].str.replace(r\"[^a-zA-Z0-9\\s]+\", \"\", regex=True)\n",
    "\n",
    "# Remove rows where the 'name' column is empty\n",
    "new_df = new_df[new_df['name'].str.strip() != \"\"]\n",
    "\n",
    "# Remove rows where rating_count_total is 0\n",
    "new_df = new_df[new_df['rating_count_tot'] != 0]\n",
    "\n",
    "# Remove rows with asian characters in the app_desc\n",
    "new_df = new_df[new_df['app_desc'].str.contains(r'[\\u4e00-\\u9fff]') == False]\n",
    "\n",
    "# sort by name\n",
    "new_df = new_df.sort_values(by='app_desc')\n",
    "\n",
    "# Display the new DataFramene\n",
    "new_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiktoken import get_encoding\n",
    "\n",
    "\n",
    "def count_tokens(text: str):\n",
    "    \"\"\"Count the number of tokens in a string\"\"\"\n",
    "    encoder = get_encoding(\"cl100k_base\")\n",
    "    return len(encoder.encode(text))\n",
    "\n",
    "\n",
    "def row_to_string(row):\n",
    "    \"\"\"Convert a row to a string\"\"\"\n",
    "    app_string = f\"\"\"AppName: {row.name}\n",
    "App Size: {round(row.size, 2)} MB\n",
    "App Price: {row.price} {row.currency}\n",
    "App Rating Count: {row.rating_count_tot}\n",
    "App User Rating: {row.user_rating}\n",
    "App Version: {row.ver}\n",
    "App Genre: {row.prime_genre}\n",
    "App Description: {row.app_desc}\"\"\"\n",
    "    return app_string\n",
    "\n",
    "\n",
    "def slice_df_by_tokens(df: pd.DataFrame, max_total_tokens: int):\n",
    "    \"\"\"\n",
    "    Slices a dataframe by the number of tokens.\n",
    "    \"\"\"\n",
    "    delimiter = \"\\n================\\n\"\n",
    "    app_str = \"\"\n",
    "\n",
    "    for i, row in enumerate(df.itertuples()):\n",
    "        row_str = row_to_string(row)\n",
    "        num_tokens = count_tokens(f\"{app_str}{delimiter}{row_str}\")\n",
    "        if num_tokens < max_total_tokens:  # If we havent hit the token limit, add the row\n",
    "            app_str += f\"{row_str}{delimiter}\"\n",
    "        else:\n",
    "            return app_str, df[:i]\n",
    "\n",
    "\n",
    "def get_names(df: pd.DataFrame):\n",
    "    return [name.lower() for name in df['name'].tolist()]\n",
    "\n",
    "\n",
    "def get_biggest_apps(df: pd.DataFrame, top_n: int = 5):\n",
    "    \"\"\"\n",
    "    Get the top n apps by size\n",
    "    \"\"\"\n",
    "    sorted_df = df.sort_values(by='size', ascending=False)\n",
    "    sliced_df = sorted_df[:top_n]\n",
    "    # Return list of app names\n",
    "    return get_names(sliced_df)\n",
    "\n",
    "\n",
    "def get_most_ratings(df: pd.DataFrame, top_n: int = 5):\n",
    "    \"\"\"\n",
    "    Get the top n apps by rating count\n",
    "    \"\"\"\n",
    "    sorted_df = df.sort_values(by='rating_count_tot', ascending=False)\n",
    "    sliced_df = sorted_df[:top_n]\n",
    "    # Return list of app names\n",
    "    return get_names(sliced_df)\n",
    "\n",
    "\n",
    "def get_best_rated(df: pd.DataFrame, top_n: int = 5):\n",
    "    \"\"\"\n",
    "    Get the top n apps by user rating count and user rating\n",
    "    \"\"\"\n",
    "    sorted_df = df.sort_values(\n",
    "        by=['rating_count_tot', 'user_rating'], ascending=False)\n",
    "    sliced_df = sorted_df[:top_n]\n",
    "    # Return list of app names\n",
    "    return get_names(sliced_df)\n",
    "\n",
    "\n",
    "def get_accurate_report(tokens: int, top_n=5):\n",
    "    _, df = slice_df_by_tokens(new_df, tokens)\n",
    "    biggest_apps = get_biggest_apps(df, top_n)\n",
    "    most_rated_apps = get_most_ratings(df, top_n)\n",
    "    best_rated_apps = get_best_rated(df, top_n)\n",
    "    return {\"biggest_apps\": biggest_apps, \"most_rated_apps\": most_rated_apps, \"best_rated_apps\": best_rated_apps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ReportForm(BaseModel):\n",
    "    \"\"\"A report of the biggest apps, the most rated apps, and the best rated apps\"\"\"\n",
    "\n",
    "    biggest_apps: List[str] = Field(\n",
    "        description=\"The list of app names in the ## Biggest Apps section of the report\"\n",
    "    )\n",
    "    # most_rated_apps: List[str] = Field(\n",
    "    #     description=\"The list of app names in the ## Most Rated Apps section of the report\"\n",
    "    # )\n",
    "    # best_rated_apps: List[str] = Field(\n",
    "    #     description=\"The list of app names in the ## Best Rated Apps section of the report\"\n",
    "    # )\n",
    "\n",
    "\n",
    "def llm_parse_report(text: str, max_tokens=2000):\n",
    "    \"\"\"Use an LLM to parse the names from the LLM output to an array\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.5,\n",
    "                     max_tokens=max_tokens).with_structured_output(ReportForm)\n",
    "    system_prompt = \"\"\"The user was tasked with analyzing and writing an analysis of the biggest apps, the most rated apps, and the best rated apps. \\\n",
    "Your job is to parse the user's response and return exactly the names of the biggest apps from the report \\\n",
    "in the order that they are mentioned. Be very exact! The integrity of your response is very important. \n",
    "\n",
    "- If you are off by a single character or line item, you will be penalized!! \n",
    "- If the a section of the report doesnt exist or include anything, use an empty array.\n",
    "- Only include the names of the apps in the array. If the name includes descriptions or any other information, exclude that and only include the App Name\n",
    "\"\"\"\n",
    "    response: ReportForm = llm.invoke(\n",
    "        [SystemMessage(content=system_prompt), HumanMessage(content=text)])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(name='1M Token Steps', description='Steps for 1M token length', data_type=<DataType.kv: 'kv'>, id=UUID('ada4a79c-e1e1-4b51-adbb-86f112be6c7a'), created_at=datetime.datetime(2024, 6, 18, 20, 3, 52, 747121, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 18, 20, 3, 52, 747121, tzinfo=datetime.timezone.utc), example_count=20, session_count=16, last_session_start_time=datetime.datetime(2024, 6, 18, 23, 11, 49, 681935))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"1M Token Steps\"\n",
    "\n",
    "\n",
    "def make_dataset(increments=20):\n",
    "    \"\"\"Make and fill dataset if it doesnt exist\"\"\"\n",
    "    if client.has_dataset(dataset_name=dataset_name):\n",
    "        return client.read_dataset(dataset_name=dataset_name)\n",
    "\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name, description=\"Steps for 1M token length\")\n",
    "\n",
    "    # 1m token length\n",
    "    max_tokens = 1000000\n",
    "    steps = max_tokens // increments\n",
    "\n",
    "    size = steps\n",
    "    i = 0\n",
    "    while size <= max_tokens:\n",
    "        splits = [\"base\"]\n",
    "        if i % 2 != 0:\n",
    "            splits.append(\"even\")\n",
    "        if i < 1:\n",
    "            splits.append(\"tiny\")\n",
    "        if i < 2:\n",
    "            splits.append(\"small\")\n",
    "        if i < 5:\n",
    "            splits.append(\"medium\")\n",
    "        if i < 10:\n",
    "            splits.append(\"large\")\n",
    "\n",
    "        client.create_example(\n",
    "            inputs={\"tokens\": size}, split=splits, dataset_name=dataset.name)\n",
    "\n",
    "        size += steps\n",
    "        i += 1\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = make_dataset()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camronhaider/Documents/Dev/AI-Research/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "top_n = 5\n",
    "\n",
    "# 2. Top {top_n} Most Rated Apps - List of the names of the top {top_n} most rated apps in the dataset in descending order by highest Rating Count.\n",
    "# 3. Top {top_n} Best Rated Apps - List of the names of the top {top_n} best rated apps in the dataset in descending order by highest User Rating and \\\n",
    "\n",
    "# ## Top {top_n} Most Rated Apps by Rating Count\n",
    "# 1. AppName\n",
    "# 2. ...\n",
    "\n",
    "# ## Top {top_n} Best Rated Apps by User Rating and Rating Count\n",
    "# 1. AppName\n",
    "# 2. ...\n",
    "\n",
    "\n",
    "def predict(inputs: dict):\n",
    "    token_count: int = inputs[\"tokens\"]\n",
    "    app_data_str, df = slice_df_by_tokens(new_df, token_count)\n",
    "    system_prompt = f\"\"\"You are an expert data analyzer. Your job is to review the content below in <APP STORE DATA> and produce a report that \\\n",
    "includes the following sections:\n",
    "\n",
    "1. Top {top_n} Biggest Apps - List the names of the top {top_n} biggest apps in the dataset in descending order by Size.\n",
    "\n",
    "You will be graded on precision, recall, and order correctness. Be sure to be as accurate as possible or you will be penalized!! ONLY use the \\\n",
    "data provided in the <APP STORE DATA> section. Do not make up any data or use any other data or knowledge to make your report or you will be PENALIZED! \\\n",
    "ONLY provide the name of the app\n",
    "\n",
    "Here is the format you should respond in:\n",
    "\n",
    "```\n",
    "# Thoughts\n",
    "[Think through your reasoning. Make decisions here and write about your through process step by step to reach a conclusion about the data. Deliberate on \\\n",
    "your options within the data. We need to know how you came to your conclusion, be verbose and mention real data points.]\n",
    "\n",
    "# App Store Data Report\n",
    "\n",
    "## Top {top_n} Biggest Apps Ordered by Size\n",
    "1. AppName\n",
    "2. ...\n",
    "```\n",
    "\n",
    "<APP STORE DATA>\n",
    "{app_data_str}\n",
    "</APP STORE DATA>\"\"\"\n",
    "    report_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "    response = report_llm.invoke(system_prompt)\n",
    "    print(\"Gemini Responded!\")\n",
    "    report: ReportForm = llm_parse_report(response.content)\n",
    "    print(response.content)\n",
    "    expected_report = get_accurate_report(token_count)\n",
    "    return {\"report\": report.dict(), \"expected_report\": expected_report}\n",
    "\n",
    "\n",
    "# predict({\"tokens\": 4000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "sections = [\"biggest_apps\"]\n",
    "\n",
    "if not top_n:\n",
    "    top_n = 5\n",
    "\n",
    "total_answers = top_n * len(sections)\n",
    "\n",
    "\n",
    "def get_score_card():\n",
    "    \"\"\"Get empty scorecard\"\"\"\n",
    "    return {\n",
    "        \"biggest_apps\": {\"correct\": 0, \"incorrect_answers\": []},\n",
    "        \"most_rated_apps\": {\"correct\": 0, \"incorrect_answers\": []},\n",
    "        \"best_rated_apps\": {\"correct\": 0, \"incorrect_answers\": []}\n",
    "    }\n",
    "\n",
    "\n",
    "def precision(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"The LLM's ability to only recall real titles from the expected list\"\"\"\n",
    "    report = root_run.outputs[\"report\"]\n",
    "    tokens = example.inputs[\"tokens\"]\n",
    "\n",
    "    if not report:\n",
    "        return {\"score\": 0, \"key\": \"precision\", \"comment\": \"No report provided\"}\n",
    "\n",
    "    expected_report = get_accurate_report(tokens)\n",
    "    scorecard = get_score_card()\n",
    "\n",
    "    # For every name in each section of the report. If that name exists in that section of the expected report, its correct\n",
    "    for section in sections:\n",
    "        for name in report[section]:\n",
    "            if name.lower() in expected_report[section]:\n",
    "                scorecard[section][\"correct\"] += 1\n",
    "            else:\n",
    "                scorecard[section][\"incorrect_answers\"].append(name)\n",
    "\n",
    "    scores = []\n",
    "    for section in sections:\n",
    "        final_score = scorecard[section][\"correct\"] / top_n\n",
    "        # make comment\n",
    "        if scorecard[section][\"incorrect_answers\"]:\n",
    "            comment = f\"Incorrect Answers in {section}: {', '.join(scorecard[section]['incorrect_answers'])}\"\n",
    "        else:\n",
    "            comment = None\n",
    "\n",
    "        scores.append(\n",
    "            {\"score\": final_score, \"key\": \"precision\", \"comment\": comment})\n",
    "\n",
    "    return {\"results\": scores}\n",
    "\n",
    "\n",
    "def recall(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"The LLM's ability to recall all real titles from the expected list\"\"\"\n",
    "    report = root_run.outputs[\"report\"]\n",
    "    tokens = example.inputs[\"tokens\"]\n",
    "\n",
    "    if not report:\n",
    "        return {\"score\": 0, \"key\": \"precision\", \"comment\": \"No report provided\"}\n",
    "\n",
    "    expected_report = get_accurate_report(tokens)\n",
    "    scorecard = get_score_card()\n",
    "\n",
    "    for section in sections:\n",
    "        for name in expected_report[section]:\n",
    "            lowered_section = [n.lower() for n in report[section]]\n",
    "            if name in lowered_section:\n",
    "                scorecard[section][\"correct\"] += 1\n",
    "            else:\n",
    "                scorecard[section][\"incorrect_answers\"].append(name)\n",
    "\n",
    "    scores = []\n",
    "    for section in sections:\n",
    "        final_score = scorecard[section][\"correct\"] / top_n\n",
    "        # make comment\n",
    "        if scorecard[section][\"incorrect_answers\"]:\n",
    "            comment = f\"Missed Apps in {section}: {', '.join(scorecard[section]['incorrect_answers'])}\"\n",
    "        else:\n",
    "            comment = None\n",
    "        scores.append(\n",
    "            {\"score\": final_score, \"key\": \"recall\", \"comment\": comment})\n",
    "\n",
    "    return {\"results\": scores}\n",
    "\n",
    "\n",
    "def order(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"The LLM's ability to order the titles correctly\"\"\"\n",
    "    report = root_run.outputs[\"report\"]\n",
    "    tokens = example.inputs[\"tokens\"]\n",
    "\n",
    "    if not report:\n",
    "        return {\"score\": 0, \"key\": \"precision\", \"comment\": \"No report provided\"}\n",
    "\n",
    "    expected_report = get_accurate_report(tokens)\n",
    "    scorecard = get_score_card()\n",
    "\n",
    "    for section in sections:\n",
    "        for i, name in enumerate(expected_report[section]):\n",
    "            if name.lower() == report[section][i].lower():\n",
    "                scorecard[section][\"correct\"] += 1\n",
    "            else:\n",
    "                scorecard[section][\"incorrect_answers\"].append(name)\n",
    "                break\n",
    "\n",
    "    scores = []\n",
    "    for section in sections:\n",
    "        final_score = scorecard[section][\"correct\"] / top_n\n",
    "        if scorecard[section][\"incorrect_answers\"]:\n",
    "            comment = f\"First apps to be out of order in {section}: {', '.join(scorecard[section]['incorrect_answers'])}\"\n",
    "        else:\n",
    "            comment = None\n",
    "        scores.append(\n",
    "            {\"score\": final_score, \"key\": \"order\", \"comment\": comment})\n",
    "\n",
    "    return {\"results\": scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'advanced-change-18' at:\n",
      "https://smith.langchain.com/o/d967989d-4221-53db-b0a5-665b504acba2/datasets/ada4a79c-e1e1-4b51-adbb-86f112be6c7a/compare?selectedSessions=afe785c7-0699-4e2f-afd8-673796472ccc\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Responded!\n",
      "```\n",
      "# Thoughts\n",
      "I will go through the data and identify the App Size of each app. Then I will sort the apps by size in descending order, and list the top 5.\n",
      "\n",
      "The top 5 app sizes are:\n",
      "1. Flight Unlimited 2K16  Flight Simulator (2860.06 MB)\n",
      "2. Broken Age  (2756.35 MB)\n",
      "3. XCOM Enemy Within (3346.28 MB)\n",
      "4. FINAL FANTASY  (3681.57 MB)\n",
      "5. Bully Anniversary Edition (2301.59 MB)\n",
      "\n",
      "# App Store Data Report\n",
      "\n",
      "## Top 5 Biggest Apps Ordered by Size\n",
      "1. XCOM Enemy Within\n",
      "2. FINAL FANTASY \n",
      "3. Flight Unlimited 2K16  Flight Simulator\n",
      "4. Broken Age \n",
      "5. Bully Anniversary Edition\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:01, 61.54s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExperimentResults advanced-change-18>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import evaluate\n",
    "\n",
    "evaluate(\n",
    "    predict,\n",
    "    data=client.list_examples(dataset_id=dataset.id, splits=[\"tiny\"]),\n",
    "    evaluators=[precision, recall, order],\n",
    "    max_concurrency=1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
