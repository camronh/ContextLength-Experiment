{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Length Experiment\n",
    "\n",
    "This notebook is to experiment how well ultra-long context LLMs can perform reasoning tasks over their context length. \n",
    "\n",
    "## Observation\n",
    "\n",
    "Needle-in-a-haystack (NIH) tests test the ability of the LLM to find factoids that have been sprinkled randomly into the context. For example they might put things like \"Pineapple is the best pizza topping\" somewhere in all of the Paul Graham essays, then ask the LLM what is the best pizza topping. This is great for measuring how well the LLM can recall facts from its context but it doesnt measure how well the LLM can reason over 1M tokens of context.\n",
    "\n",
    "This experiment will test that ability. We will fill the context window up gradually and measure how well the LLM can answer questions that require it to understand the entire context. \n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "My guess is that the agent will be able to perform these tasks with high accuracy in low context lengths, and the accuracy will drop off after a point. I also believe that the LLM will be close to accurate every time, but will not be 100% accurate after a certain threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "The way we will be able to evaluate the accuracy is by choosing tasks that we can validate programattically. So we cant just ask it for an analysis because we wont be able to descreetly validate the analysis results. We also want to avoid adding variables, so we won't want to include any tests that guage the ability of the LLM to count or function call for example. So in that case I thought of a few experiments:\n",
    "\n",
    "1. Parse Titles - We ask the LLM to give us a list of all of the titles of all of the essays in order. We can write some regex scripts to parse that info ourselves and validate precision and recall and order correctness.\n",
    "2. Parse Quotes - We ask the LLM to give us a list of all of the quotes that Paul Graham includes in his essays. We can then write regex to parse out everything wrapped in quotes (or `blockquotes`) and validate precision and recall\n",
    "3. Ordered Instructions - We write a set of step by step instructions and break those steps up and sprinkle them randomly in the essays in order. We then ask the LLM to put those instructions together into the correct order and validate the accuracy.\n",
    "4. Unordered Instructions - Same as the previous experiment but mix the order of the steps.\n",
    "5. Parse Links - Parse out all of the href links from the essays and validate the accuracy of the LLM in parsing out the links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def split_essays():\n",
    "    \"\"\"Split the Paul Graham essays\"\"\"\n",
    "    with open(\"./paul_graham_essay.txt\", \"r\") as file:\n",
    "        essay_text = file.read()\n",
    "\n",
    "    essays = []\n",
    "    lines = []\n",
    "    # Regex to match titles formatted as \"Month Year\"\n",
    "    title_pattern = re.compile(r'^(January|February|March|April|May|June|July|August|September|October|November|December) \\d{4}$')\n",
    "\n",
    "    for line in essay_text.split('\\n'):\n",
    "        if title_pattern.match(line.strip()):\n",
    "            # If we find a title and have collected lines for an essay, save the essay\n",
    "            if lines:\n",
    "                essays.append(\"\\n\".join(lines).strip())\n",
    "                lines = []\n",
    "        lines.append(line)\n",
    "\n",
    "    # Add the last essay collected, if any\n",
    "    if lines:\n",
    "        essays.append(\"\\n\".join(lines).strip())\n",
    "\n",
    "    return essays\n",
    "\n",
    "essays = split_essays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(essays[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Titles Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "\n",
    "We need to create a dataset that includes a list of indexes as the input and the expected titles in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Example\n",
    "\n",
    "def parse_titles(essays: list[str]):\n",
    "    titles = []\n",
    "    for essay in essays:\n",
    "        title = essay.split(\"\\n\")[0]\n",
    "        titles.append(title)\n",
    "    return titles\n",
    "\n",
    "\n",
    "def create_titles_dataset(essays=essays, chunks=8):\n",
    "    examples = []\n",
    "    for index in range(0, len(essays), chunks):\n",
    "        # Current index and all indexes before it\n",
    "        indexes = [i for i in range(index)]\n",
    "        essays_to_parse = [essays[i] for i in indexes]\n",
    "\n",
    "        if not indexes:\n",
    "            continue\n",
    "\n",
    "        examples.append({\"inputs\": {\"indexes\": indexes}, \"outputs\": {\"titles\": parse_titles(essays=essays_to_parse)}})\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = create_titles_dataset()\n",
    "len(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets upload them to a dataset in langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langsmith import Client\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def upload_title_dateset():\n",
    "    client = Client()\n",
    "    dataset_name = \"Context Experiment - Titles\"\n",
    "\n",
    "    # Storing inputs in a dataset lets us\n",
    "    # run chains and LLMs over a shared set of examples.\n",
    "    # dataset = client.create_dataset(\n",
    "    #     dataset_name=dataset_name,\n",
    "    #     description=\"Experiment to test the ability of the LLM to recall titles from essays.\",\n",
    "    # )\n",
    "\n",
    "    for i, example in enumerate(examples):\n",
    "        client.create_example(\n",
    "            inputs=example[\"inputs\"],\n",
    "            outputs=example[\"outputs\"],\n",
    "            dataset_name=dataset_name,\n",
    "            metadata={\"index\": i}\n",
    "        )\n",
    "\n",
    "    # return dataset\n",
    "\n",
    "upload_title_dateset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now lets write the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-openai langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.schema.messages import SystemMessage, HumanMessage\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class TitleSchema(BaseModel):\n",
    "    \"\"\"A list of ALL titles, including duplicates, in the order that they appear in the response.\"\"\"\n",
    "\n",
    "    titles: List[str] = Field(\n",
    "        description=\"A list of ALL titles, including duplicates, in the order that they appear in the response.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def llm_parse_titles(text: str):\n",
    "    \"\"\"Use an LLM to parse the titles from the LLM output to an array\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(TitleSchema)\n",
    "    system_prompt = \"\"\"The user was tasked with analyzing and writing a list of titles that they found in some essays. \\\n",
    "Your job is to parse the user's response and return a list of titles that they mentioned. Be very exact! The integrity of your \\\n",
    "response is very important. If you are off by a single character, you will be penalized.\"\"\"\n",
    "    response: TitleSchema = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=text)])\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets write the evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "\n",
    "def title_precision(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"The LLM's ability to only recall real titles from the expected list\"\"\"\n",
    "    output_titles: list[str] = root_run.outputs.get(\"titles\", [])\n",
    "    expected_titles: list[str] = example.outputs[\"titles\"]\n",
    "\n",
    "\n",
    "    if not output_titles:\n",
    "        return {\"score\": 0, \"key\": \"precision\", \"comment\": \"No output titles provided\"}\n",
    "\n",
    "    score = 0\n",
    "    false_positives = []\n",
    "    for llm_title in output_titles:\n",
    "        if llm_title in expected_titles:\n",
    "            score += 1\n",
    "        else:\n",
    "            false_positives.append(llm_title)\n",
    "\n",
    "    final_score = score / len(output_titles) if output_titles else 0.0\n",
    "    comment = f\"Titles not included in the example: {', '.join(false_positives)}\"\n",
    "    return {\"score\": final_score, \"key\": \"precision\", \"comment\": comment}\n",
    "\n",
    "\n",
    "def title_recall(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"The LLM's ability to recall all real titles from the expected list\"\"\"\n",
    "    output_titles: list[str] = root_run.outputs.get(\"titles\", [])\n",
    "    expected_titles: list[str] = example.outputs[\"titles\"]\n",
    "\n",
    "    total_expected_titles = len(expected_titles)\n",
    "    output_titles_copy = output_titles.copy()\n",
    "    score = 0\n",
    "    missed_titles = []\n",
    "\n",
    "    for expected_title in expected_titles:\n",
    "        if expected_title in output_titles_copy:\n",
    "            score += 1\n",
    "            output_titles_copy.remove(expected_title)  # Remove the title to account for duplicates\n",
    "        else:\n",
    "            missed_titles.append(expected_title)\n",
    "\n",
    "    final_score = score / total_expected_titles if total_expected_titles else 0.0\n",
    "    comment = f\"Missed titles from the expected list: {', '.join(missed_titles)}\"\n",
    "    return {\"score\": final_score, \"key\": \"recall\", \"comment\": comment}\n",
    "\n",
    "\n",
    "def title_order(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"The LLM's ability to order the titles correctly\"\"\"\n",
    "    output_titles: list[str] = root_run.outputs.get(\"titles\", [])\n",
    "    expected_titles: list[str] = example.outputs[\"titles\"]\n",
    "    score = 0\n",
    "    out_of_order_title = None\n",
    "\n",
    "    for index, title in enumerate(expected_titles):\n",
    "        try:\n",
    "            if title.lower() == output_titles[index].lower():\n",
    "                score += 1\n",
    "            else:\n",
    "                out_of_order_title = title\n",
    "                break\n",
    "        except:\n",
    "            out_of_order_title = title\n",
    "            break\n",
    "\n",
    "    final_score = score / len(output_titles) if output_titles else 0.0\n",
    "    comment = (\n",
    "        f\"First title out of order: {out_of_order_title}\"\n",
    "        if out_of_order_title\n",
    "        else \"All titles are in the correct order\"\n",
    "    )\n",
    "    return {\"score\": final_score, \"key\": \"order\", \"comment\": comment}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay this seems good. Lets put it all together into an eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langsmith import evaluate, Client\n",
    "\n",
    "client = Client()\n",
    "dataset_id = \"e4c55781-52d9-47e7-a1bb-fcdac719d838\"\n",
    "\n",
    "dataset = client.read_dataset(dataset_id=dataset_id)\n",
    "\n",
    "\n",
    "class GeminiPredictor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm=ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-flash\", temperature=0, google_api_key=\"**\"\n",
    "        ),\n",
    "        prefix=\"Flash\",\n",
    "    ):\n",
    "        self.llm = llm\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def predict_titles(self, inputs: dict):\n",
    "        print(\"PREDICTING...\")\n",
    "        indexes: list[int] = inputs[\"indexes\"]\n",
    "\n",
    "        essays_for_context = [essays[i] for i in indexes]\n",
    "        essay_str = \"\\n\\n\".join(essays_for_context)\n",
    "\n",
    "        system_prompt = f\"\"\"You are a very thorough and detailed analyzer of Paul Graham essays. Your task it to analyze the \\\n",
    "provided essays and ONLY return a single list of titles in the order that they appear. You can tell which lines are titles because the \\\n",
    "contain ONLY a month and year followed by 2 new lines. For example: 'February 1993\\n\\n'. \n",
    "\n",
    "Return a numbered list of all of the titles you have found. \\\n",
    "You will be graded on precision and recall so be sure to include ALL of the titles and in the correct order. Some duplicates are expected, \\\n",
    "just be sure to be as accurate as possible!\n",
    "\n",
    "\n",
    "<PAUL GRAHAM ESSAYS>\n",
    "{essay_str}\n",
    "</PAUL GRAHAM ESSAYS>\"\"\"\n",
    "        response = self.llm.invoke(system_prompt)\n",
    "        titles = llm_parse_titles(response.content)\n",
    "        return {\"response\": response.content, \"titles\": titles.titles}\n",
    "\n",
    "    def evaluate(self, splits: list[str] = [\"tiny\"], max_concurrency: int = 1, repetitions=1):\n",
    "        examples = client.list_examples(dataset_name=\"Context Experiment - Titles\", splits=splits)\n",
    "        for example in examples:\n",
    "            essay_count = len(example.inputs[\"indexes\"])\n",
    "            evaluate(\n",
    "                self.predict_titles,\n",
    "                data=client.list_examples(\n",
    "                    dataset_name=\"Context Experiment - Titles\", splits=splits, metadata={\"index\": example.metadata[\"index\"]}\n",
    "                ),\n",
    "                evaluators=[title_precision, title_recall, title_order],\n",
    "                experiment_prefix=f\"{self.prefix}-{essay_count}_Essays\",\n",
    "                max_concurrency=max_concurrency,\n",
    "                num_repetitions=repetitions,\n",
    "            )\n",
    "\n",
    "\n",
    "# response = GeminiPredictor().predict_titles({\"indexes\": [0, 1]})\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = GeminiPredictor()\n",
    "predictor.evaluate(splits=[\"small5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_examples = client.list_examples(dataset_name=\"Context Experiment - Titles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_examples = client.list_examples(dataset_name=\"Context Experiment - Titles\")\n",
    "for example in total_examples:\n",
    "    print(example.metadata[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response\n",
    "print(response[\"titles\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import evaluate, Client\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_id = \"e4c55781-52d9-47e7-a1bb-fcdac719d838\"\n",
    "\n",
    "dataset = client.read_dataset(dataset_id=dataset_id)\n",
    "\n",
    "examples = client.list_examples(dataset_name=dataset.name, splits=[\"single\"])\n",
    "\n",
    "evaluate(\n",
    "    predictor.predict_titles,\n",
    "    data=examples,\n",
    "    evaluators=[title_precision, title_recall, title_order],\n",
    "    experiment_prefix=\"flash\",\n",
    "    max_concurrency=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
